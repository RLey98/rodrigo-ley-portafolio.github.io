<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Inappropiate Comments - Model for Classifying Toxic Comments</title>
    <style>
      body {
        font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
          Roboto, sans-serif;
        line-height: 1.6;
        color: #333;
        max-width: 900px;
        margin: 0 auto;
        padding: 2rem;
      }

      h1 {
        font-size: 2.5rem;
        margin-bottom: 1.5rem;
      }
      h2 {
        font-size: 2rem;
        margin-top: 2rem;
        margin-bottom: 1rem;
      }
      h3 {
        font-size: 1.5rem;
        margin-top: 1.5rem;
        margin-bottom: 0.8rem;
      }

      img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 1.5rem 0;
      }

      :root {
        --bg-color: #2a2a2a;
        --second-bg-color: #202020;
        --text-color: #fff;
        --second-color: #ccc;
        --main-color: #ff4d05;
        --big-font: 5rem;
        --h2-font: 3rem;
        --p-font: 1.1rem;
      }
      body {
        background: var(--bg-color);
        color: var(--text-color);
      }
      p {
        margin: 1rem 0;
      }

      /* Estilo para ‚Äúsecci√≥n‚Äù */
      .section {
        margin-bottom: 2rem;
      }

      .section-intro {
        margin-bottom: 3rem;
      }
    </style>
  </head>
  <body>
    <div style="margin-top: 40px">
      <a
        href="https://rley98.github.io/rodrigo-ley-portafolio.github.io/"
        style="
          background-color: #ff4d05;
          color: white;
          padding: 12px 28px;
          border-radius: 8px;
          text-decoration: none;
          font-weight: 600;
          font-family: 'Inter', sans-serif;
          transition: all 0.3s ease;
        "
      >
        ‚Üê Back to Projects
      </a>
    </div>
    <header class="section section-intro">
      <h1>Inappropiate Comments - Model for Classifying Toxic Comments</h1>
    </header>

    <section class="section">
      <h2>üîπ Context</h2>
      <p>
        Online communities often face the challenge of toxic or offensive
        comments that harm user experience and increase moderators‚Äô workload.
      </p>
      <p>
        <strong>Main question:</strong> How can we automatically detect
        inappropriate comments before they are published?
      </p>
    </section>

    <section class="section">
      <h2>üéØ Project Goal</h2>
      <p>
        Develop a system capable of identifying comments that contain offensive,
        hateful, or threatening language, helping online platforms moderate
        discussions faster and more accurately.
      </p>
    </section>

    <section class="section">
      <h2>ü™ú Approach</h2>
      <p>
        To tackle this problem, I followed a process focused on understanding
        the language behind toxicity:
      </p>

      <h3>üìò Data Exploration</h3>
      <p>
        I analyzed a real dataset of user comments labeled as <em>toxic</em>,
        <em>offensive</em>, or <em>neutral</em>. This helped identify common
        patterns and frequent words across different categories.
      </p>

      <h3>üßπ Text Cleaning</h3>
      <p>
        I removed unnecessary characters, links, symbols, and redundant
        punctuation so that the analysis focused purely on the message content.
      </p>

      <h3>üåà Visual Pattern Discovery</h3>
      <p>
        I created visualizations and word clouds to uncover the most frequent
        expressions in toxic comments. For example:
      </p>
      <ul>
        <li>Short and repetitive insults were the most common.</li>
        <li>
          Threats often included words related to violence or physical harm.
        </li>
      </ul>

      <h3>ü§ñ Model Training</h3>
      <p>
        The cleaned text was converted into numerical features (like word
        frequency), and a
        <strong>multi-label classification model</strong> was trained to predict
        the toxicity level of each comment.
      </p>
    </section>
    <section class="section">
      <h2>üìä Visualizations with Simple Explanations</h2>

      <h3>üî∏ Distribution of Toxicity Types</h3>
      <p>
        This chart shows how many comments belong to each category. It reveals
        that <strong>insults and offensive remarks are the most common</strong>,
        while explicit threats make up a smaller portion.
      </p>
      <p style="text-align: center; font-style: italic">
        <img src="./images/toxic_count.png" alt="Descripci√≥n de imagen 2" />
      </p>

      <h3>üî∏ Word Cloud ‚Äì Offensive Comments</h3>
      <p>
        This word cloud highlights the most frequent terms found in offensive
        comments. Words like <em>‚Äúidiot‚Äù</em>, <em>‚Äústupid‚Äù</em>, and
        <em>‚Äúhate‚Äù</em> appear often, showing clear patterns of aggressive
        language.
      </p>
      <p style="text-align: center; font-style: italic">
        <img src="./images/wordscloud.png" alt="Descripci√≥n de imagen 3" />
      </p>
    </section>
    <section class="section">
      <h2>‚úÖ Results</h2>
      <p>
        The final model achieved the following ROC AUC scores for each class:
      </p>

      <table style="width: 100%; border-collapse: collapse; margin: 1em 0">
        <thead style="background-color: #000000">
          <tr>
            <th style="border: 1px solid #ccc; padding: 8px">Category</th>
            <th style="border: 1px solid #ccc; padding: 8px">ROC AUC</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border: 1px solid #ccc; padding: 8px">Toxic</td>
            <td style="border: 1px solid #ccc; padding: 8px">0.879</td>
          </tr>
          <tr>
            <td style="border: 1px solid #ccc; padding: 8px">Severe Toxic</td>
            <td style="border: 1px solid #ccc; padding: 8px">0.875</td>
          </tr>
          <tr>
            <td style="border: 1px solid #ccc; padding: 8px">Obscene</td>
            <td style="border: 1px solid #ccc; padding: 8px">0.919</td>
          </tr>
          <tr>
            <td style="border: 1px solid #ccc; padding: 8px">Threat</td>
            <td style="border: 1px solid #ccc; padding: 8px">0.970</td>
          </tr>
          <tr>
            <td style="border: 1px solid #ccc; padding: 8px">Insult</td>
            <td style="border: 1px solid #ccc; padding: 8px">0.882</td>
          </tr>
          <tr>
            <td style="border: 1px solid #ccc; padding: 8px">Identity Hate</td>
            <td style="border: 1px solid #ccc; padding: 8px">0.886</td>
          </tr>
        </tbody>
      </table>

      <p>
        These results show strong performance in detecting various types of
        toxicity, especially in identifying
        <em>threats</em> and <em>obscene language</em>. This allows moderators
        to automate a large portion of the review process, significantly
        reducing manual work.
      </p>
    </section>
    <section class="section">
      <h2>üîó View the Code</h2>
      <p>
        üíª View the full code and notebook on GitHub:
        <a
          href="https://github.com/RLey98/Inappropiate-Comments-Classification"
          style="color: #007acc; text-decoration: none"
          ><strong>üëâ Click here to open the repository</strong></a
        >
      </p>
    </section>
  </body>
</html>
